"""Enhanced test generation for MCP servers.

Generates comprehensive tests including:
- Tool existence tests
- Functional tests with sample inputs
- Error case tests
- Boundary condition tests
- Input validation tests
"""

from dataclasses import dataclass, field
from typing import Any
import json


@dataclass
class GeneratedTestCase:
    """A single test case."""
    name: str
    description: str
    tool_name: str
    inputs: dict[str, Any]
    expected_success: bool = True
    expected_error: str | None = None
    check_output: bool = False
    output_contains: str | None = None


# Alias for backward compatibility
TestCase = GeneratedTestCase


@dataclass
class GeneratorConfig:
    """Configuration for test generation."""
    generate_existence_tests: bool = True
    generate_functional_tests: bool = True
    generate_error_tests: bool = True
    generate_boundary_tests: bool = True
    generate_validation_tests: bool = True
    use_mocking: bool = True
    async_tests: bool = True


# Alias for backward compatibility
TestGeneratorConfig = GeneratorConfig


class EnhancedTestGenerator:
    """Generate comprehensive tests for MCP servers."""

    def __init__(self, config: TestGeneratorConfig | None = None):
        self.config = config or TestGeneratorConfig()

    def generate_tests(
        self,
        server_name: str,
        tool_specs: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive test file.

        Args:
            server_name: Name of the server
            tool_specs: List of tool specifications

        Returns:
            Python test file contents
        """
        parts = [
            self._generate_header(server_name),
            self._generate_imports(),
            self._generate_fixtures(),
        ]

        if self.config.generate_existence_tests:
            parts.append(self._generate_existence_tests(tool_specs))

        if self.config.generate_functional_tests:
            parts.append(self._generate_functional_tests(tool_specs))

        if self.config.generate_error_tests:
            parts.append(self._generate_error_tests(tool_specs))

        if self.config.generate_boundary_tests:
            parts.append(self._generate_boundary_tests(tool_specs))

        if self.config.generate_validation_tests:
            parts.append(self._generate_validation_tests(tool_specs))

        return "\n".join(parts)

    def _generate_header(self, server_name: str) -> str:
        """Generate file header."""
        return f'''"""Comprehensive tests for {server_name}.

Auto-generated by MCP Tool Factory.
Includes existence, functional, error, boundary, and validation tests.
"""

'''

    def _generate_imports(self) -> str:
        """Generate import statements."""
        imports = [
            "import json",
            "import pytest",
            "from unittest.mock import AsyncMock, MagicMock, patch",
        ]

        if self.config.async_tests:
            imports.append("import asyncio")

        return "\n".join(imports) + "\n\n"

    def _generate_fixtures(self) -> str:
        """Generate test fixtures."""
        return '''
# ============== FIXTURES ==============

@pytest.fixture
def mock_mcp_server():
    """Create a mock MCP server for testing."""
    from server import mcp
    return mcp


@pytest.fixture
async def mcp_client(mock_mcp_server):
    """Create MCP client connected to server."""
    from mcp import Client

    async with Client(transport=mock_mcp_server) as client:
        yield client


@pytest.fixture
def tool_registry():
    """Get registered tools from server."""
    from server import mcp
    return {tool.name: tool for tool in mcp.list_tools()}


'''

    def _generate_existence_tests(self, tool_specs: list[dict]) -> str:
        """Generate tool existence tests."""
        code = '''
# ============== EXISTENCE TESTS ==============

class TestToolExistence:
    """Verify all tools are properly registered."""

    @pytest.mark.asyncio
    async def test_all_tools_registered(self, mcp_client):
        """Test all expected tools are registered."""
        tools = await mcp_client.list_tools()
        tool_names = {t.name for t in tools.tools}

'''
        # Add assertions for each tool
        for spec in tool_specs:
            name = spec.get("name", "unknown")
            code += f'        assert "{name}" in tool_names, "Tool {name} not registered"\n'

        # Add individual tool tests
        for spec in tool_specs:
            name = spec.get("name", "unknown")
            desc = spec.get("description", "")[:50]
            code += f'''
    @pytest.mark.asyncio
    async def test_{name}_has_description(self, mcp_client):
        """Test {name} has proper description."""
        tools = await mcp_client.list_tools()
        tool = next((t for t in tools.tools if t.name == "{name}"), None)
        assert tool is not None
        assert tool.description, "{name} should have a description"

'''
        return code

    def _generate_functional_tests(self, tool_specs: list[dict]) -> str:
        """Generate functional tests that call tools with sample inputs."""
        code = '''
# ============== FUNCTIONAL TESTS ==============

class TestToolFunctionality:
    """Test tools work with valid inputs."""

'''
        for spec in tool_specs:
            name = spec.get("name", "unknown")
            input_schema = spec.get("input_schema", spec.get("inputSchema", {}))
            sample_inputs = self._generate_sample_inputs(input_schema)

            code += f'''
    @pytest.mark.asyncio
    async def test_{name}_with_valid_input(self, mcp_client):
        """Test {name} with valid input."""
        result = await mcp_client.call_tool("{name}", {json.dumps(sample_inputs)})
        # Tool should return without error
        assert result is not None
        # Check result has expected structure
        if hasattr(result, 'content'):
            assert result.content is not None

'''
        return code

    def _generate_error_tests(self, tool_specs: list[dict]) -> str:
        """Generate error case tests."""
        code = '''
# ============== ERROR TESTS ==============

class TestToolErrors:
    """Test tools handle errors appropriately."""

    @pytest.mark.asyncio
    async def test_unknown_tool_raises(self, mcp_client):
        """Test calling unknown tool raises error."""
        with pytest.raises(Exception):
            await mcp_client.call_tool("nonexistent_tool_xyz", {})

'''
        for spec in tool_specs:
            name = spec.get("name", "unknown")
            input_schema = spec.get("input_schema", spec.get("inputSchema", {}))
            required = input_schema.get("required", [])

            if required:
                code += f'''
    @pytest.mark.asyncio
    async def test_{name}_missing_required_field(self, mcp_client):
        """Test {name} validates required fields."""
        # Missing all required fields should fail or return error
        try:
            result = await mcp_client.call_tool("{name}", {{}})
            # If it doesn't raise, check for error in result
            if hasattr(result, 'isError'):
                assert result.isError
        except Exception:
            pass  # Expected to fail

'''
        return code

    def _generate_boundary_tests(self, tool_specs: list[dict]) -> str:
        """Generate boundary condition tests."""
        code = '''
# ============== BOUNDARY TESTS ==============

class TestToolBoundaries:
    """Test tools handle boundary conditions."""

'''
        for spec in tool_specs:
            name = spec.get("name", "unknown")
            input_schema = spec.get("input_schema", spec.get("inputSchema", {}))
            properties = input_schema.get("properties", {})

            has_numbers = any(
                p.get("type") in ("number", "integer")
                for p in properties.values()
            )

            if has_numbers:
                code += f'''
    @pytest.mark.asyncio
    async def test_{name}_with_zero_values(self, mcp_client):
        """Test {name} handles zero values."""
        # Generate input with zeros for numeric fields
        inputs = {{}}
'''
                for prop_name, prop_schema in properties.items():
                    if prop_schema.get("type") in ("number", "integer"):
                        code += f'        inputs["{prop_name}"] = 0\n'
                    elif prop_schema.get("type") == "string":
                        code += f'        inputs["{prop_name}"] = "test"\n'

                code += f'''
        try:
            result = await mcp_client.call_tool("{name}", inputs)
            # Should handle zeros gracefully
            assert result is not None
        except Exception as e:
            # Some tools may reject zeros, which is acceptable
            pass

'''

            has_strings = any(
                p.get("type") == "string"
                for p in properties.values()
            )

            if has_strings:
                code += f'''
    @pytest.mark.asyncio
    async def test_{name}_with_empty_strings(self, mcp_client):
        """Test {name} handles empty strings."""
        inputs = {{}}
'''
                for prop_name, prop_schema in properties.items():
                    if prop_schema.get("type") == "string":
                        code += f'        inputs["{prop_name}"] = ""\n'
                    elif prop_schema.get("type") in ("number", "integer"):
                        code += f'        inputs["{prop_name}"] = 1\n'

                code += f'''
        try:
            result = await mcp_client.call_tool("{name}", inputs)
        except Exception:
            pass  # Empty strings may be rejected

'''
        return code

    def _generate_validation_tests(self, tool_specs: list[dict]) -> str:
        """Generate input validation tests."""
        code = '''
# ============== VALIDATION TESTS ==============

class TestInputValidation:
    """Test input validation for all tools."""

'''
        for spec in tool_specs:
            name = spec.get("name", "unknown")
            input_schema = spec.get("input_schema", spec.get("inputSchema", {}))
            properties = input_schema.get("properties", {})

            for prop_name, prop_schema in properties.items():
                prop_type = prop_schema.get("type")

                if prop_type in ("number", "integer"):
                    code += f'''
    @pytest.mark.asyncio
    async def test_{name}_{prop_name}_rejects_string(self, mcp_client):
        """Test {name} rejects string for {prop_name}."""
        inputs = {{"{prop_name}": "not_a_number"}}
        try:
            result = await mcp_client.call_tool("{name}", inputs)
            # Should fail or return error
            if hasattr(result, 'isError'):
                assert result.isError
        except Exception:
            pass  # Expected

'''

                if prop_type == "string":
                    max_length = prop_schema.get("maxLength")
                    if max_length:
                        code += f'''
    @pytest.mark.asyncio
    async def test_{name}_{prop_name}_max_length(self, mcp_client):
        """Test {name} validates maxLength for {prop_name}."""
        inputs = {{"{prop_name}": "x" * {max_length + 100}}}
        try:
            result = await mcp_client.call_tool("{name}", inputs)
            # Should fail or return error for too-long string
        except Exception:
            pass  # Expected

'''
        return code

    def _generate_sample_inputs(self, schema: dict) -> dict:
        """Generate sample inputs from a JSON schema."""
        inputs = {}
        properties = schema.get("properties", {})
        required = schema.get("required", [])

        for name, prop_schema in properties.items():
            prop_type = prop_schema.get("type", "string")

            if prop_type == "string":
                if prop_schema.get("enum"):
                    inputs[name] = prop_schema["enum"][0]
                elif prop_schema.get("format") == "email":
                    inputs[name] = "test@example.com"
                elif prop_schema.get("format") == "uri":
                    inputs[name] = "https://example.com"
                else:
                    inputs[name] = f"test_{name}"

            elif prop_type == "integer":
                minimum = prop_schema.get("minimum", 0)
                maximum = prop_schema.get("maximum", 100)
                inputs[name] = (minimum + maximum) // 2 if maximum else minimum + 1

            elif prop_type == "number":
                minimum = prop_schema.get("minimum", 0)
                maximum = prop_schema.get("maximum", 100)
                inputs[name] = (minimum + maximum) / 2 if maximum else minimum + 0.5

            elif prop_type == "boolean":
                inputs[name] = True

            elif prop_type == "array":
                items = prop_schema.get("items", {})
                if items.get("type") == "string":
                    inputs[name] = ["item1", "item2"]
                elif items.get("type") in ("integer", "number"):
                    inputs[name] = [1, 2, 3]
                else:
                    inputs[name] = []

            elif prop_type == "object":
                inputs[name] = {}

            # Only include required fields by default
            if name not in required and name in inputs:
                pass  # Keep it, good to test with all fields

        return inputs


def generate_enhanced_tests(
    server_name: str,
    tool_specs: list[dict[str, Any]],
    config: TestGeneratorConfig | None = None,
) -> str:
    """Generate comprehensive test file.

    Args:
        server_name: Name of the server
        tool_specs: List of tool specifications
        config: Optional test generation configuration

    Returns:
        Python test file contents
    """
    generator = EnhancedTestGenerator(config)
    return generator.generate_tests(server_name, tool_specs)
